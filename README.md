# ğŸ† Chess LLM Benchmark

**A comprehensive tool for evaluating Large Language Models through chess gameplay against Stockfish at various ELO ratings.**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![MIT License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4o-412991.svg)](https://openai.com/)
[![Anthropic](https://img.shields.io/badge/Anthropic-Claude--3.5-8B5A2B.svg)](https://anthropic.com/)
[![Google](https://img.shields.io/badge/Google-Gemini--1.5-4285F4.svg)](https://ai.google.dev/)

## âœ¨ Features

- ğŸ¤– **Multi-Provider Support**: OpenAI GPT models, Anthropic Claude, Google Gemini
- ğŸ¯ **ELO Ladder System**: Bots climb ratings by defeating Stockfish at increasing difficulty levels
- ğŸ¨ **Beautiful Terminal UI**: Real-time chess board visualization with rich formatting
- ğŸ“Š **Comprehensive Analytics**: Detailed statistics, win rates, and performance tracking
- ğŸ® **Preset Configurations**: Ready-made model lineups for different use cases
- ğŸ’¾ **Game Recording**: All games saved in PGN format with full analysis
- âš¡ **Concurrent Testing**: Run multiple bots simultaneously for efficient benchmarking

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/chess-llm-benchmark.git
cd chess-llm-benchmark

# Install dependencies
pip install -r requirements.txt

# Install Stockfish (required chess engine)
# macOS:
brew install stockfish

# Ubuntu/Debian:
sudo apt-get install stockfish

# Windows:
choco install stockfish
```

### Set Up API Keys

Create a `.env` file in the project root:

```env
OPENAI_API_KEY="your-openai-api-key"
ANTHROPIC_API_KEY="your-anthropic-api-key"
GEMINI_API_KEY="your-gemini-api-key"
```

### Run Your First Benchmark

```bash
# Quick demo with random bots
python main.py --demo

# Premium models from all providers
python main.py --preset premium

# Budget-friendly models
python main.py --preset budget

# OpenAI models only
python main.py --preset openai
```

## ğŸ¤– Available Models

### ğŸ“¡ OpenAI Models
- **GPT-4o** - Latest flagship model with enhanced reasoning
- **GPT-4o Mini** - Faster, cost-effective variant
- **GPT-4 Turbo** - High-performance with large context
- **GPT-3.5 Turbo** - Fast and efficient legacy model

### ğŸ“¡ Anthropic Models  
- **Claude 3.5 Sonnet** - Most intelligent Claude model
- **Claude 3.5 Haiku** - Fast and efficient
- **Claude 3 Opus** - Most capable legacy model
- **Claude 3 Haiku** - Budget-friendly option

### ğŸ“¡ Google Gemini Models
- **Gemini 1.5 Pro** - Most capable with 1M token context
- **Gemini 1.5 Flash** - Fast and efficient
- **Gemini 1.0 Pro** - Legacy model

## ğŸ¯ Preset Configurations

| Preset | Description | Models |
|--------|-------------|---------|
| `premium` | Top-tier models from each provider | GPT-4o, GPT-4o Mini, Claude 3.5 Sonnet, Claude 3.5 Haiku, Gemini 1.5 Pro, Gemini 1.5 Flash |
| `budget` | Cost-effective models with good performance | GPT-4o Mini, GPT-3.5 Turbo, Claude 3.5 Haiku, Claude 3 Haiku, Gemini 1.5 Flash |
| `recommended` | All recommended models across providers | All â­ starred models |
| `openai` | OpenAI's best models | GPT-4o, GPT-4o Mini |
| `anthropic` | Anthropic's best models | Claude 3.5 Sonnet, Claude 3.5 Haiku |
| `gemini` | Google's best models | Gemini 1.5 Pro, Gemini 1.5 Flash |

## ğŸ“‹ Usage Examples

### Basic Usage

```bash
# List all available models
python main.py --list-models

# List all presets
python main.py --list-presets

# Run with specific preset
python main.py --preset premium

# Custom bot lineup
python main.py --bots "openai:gpt-4o:GPT-4o,anthropic:claude-3-5-sonnet-20241022:Claude-3.5-Sonnet"
```

### Advanced Configuration

```bash
# Custom ELO range
python main.py --preset budget --start-elo 800 --max-elo 1600 --elo-step 200

# Faster games with shorter thinking time
python main.py --preset openai --think-time 0.1 --max-plies 100

# High-temperature creative play
python main.py --bots "openai:gpt-4o:Creative-GPT" --llm-temperature 0.8
```

### Demo Modes

```bash
# Robot battle visualization
python main.py --robot-demo

# Quick robot demo
python main.py --quick-robot-demo

# Standard demo with random bots
python main.py --demo
```

## ğŸ® Live Dashboard

The benchmark features a beautiful real-time terminal dashboard:

```
â•­â”€â”€â”€â”€â”€â”€â”€ ğŸ† Chess LLM ELO Ladder Benchmark â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚  â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ“Š Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®  â”‚
â”‚  â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”“ â”‚  â”‚
â”‚  â”‚ â”ƒ Bot              â”ƒ Max ELO â”ƒ Win Rate  â”ƒ â”‚  â”‚
â”‚  â”‚ â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”© â”‚  â”‚
â”‚  â”‚ â”‚ GPT-4o           â”‚    1400 â”‚    67.5%  â”‚ â”‚  â”‚
â”‚  â”‚ â”‚ Claude 3.5       â”‚    1200 â”‚    55.2%  â”‚ â”‚  â”‚
â”‚  â”‚ â”‚ Gemini 1.5 Pro   â”‚    1100 â”‚    48.3%  â”‚ â”‚  â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯  â”‚
â”‚  â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GPT-4o â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®  â”‚
â”‚  â”‚ Ladder: 600 â†’ 800 â†’ 1000 â†’ 1200 â†’ 1400     â”‚  â”‚
â”‚  â”‚ â•­â”€ Move 24 | Last: Nf6+ â”€â”€â•®Status: vs 1400â”‚  â”‚
â”‚  â”‚ â”‚  a b c d e f g h       â”‚Current ELO:    â”‚  â”‚
â”‚  â”‚ â”‚8 â™œ   â™ â™› â™š â™   â™œ 8     â”‚1400           â”‚  â”‚
â”‚  â”‚ â”‚7 â™Ÿ â™Ÿ   â™Ÿ   â™Ÿ â™Ÿ â™Ÿ 7     â”‚Color: White   â”‚  â”‚
â”‚  â”‚ â”‚6       â™           6     â”‚Moves: 47      â”‚  â”‚
â”‚  â”‚ â”‚5         â™™         5     â”‚               â”‚  â”‚
â”‚  â”‚ â”‚4   â™™     â™™         4     â”‚Win Rate: 67.5%â”‚  â”‚
â”‚  â”‚ â”‚3     â™˜   â™—         3     â”‚Games: 8       â”‚  â”‚
â”‚  â”‚ â”‚2 â™™     â™™   â™™ â™™ â™™   2     â”‚Record: 5W-1D- â”‚  â”‚
â”‚  â”‚ â”‚1 â™– â™˜ â™— â™• â™”     â™–   1     â”‚2L            â”‚  â”‚
â”‚  â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯               â”‚  â”‚
â”‚  â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```

## ğŸ“Š Results and Analysis

### Output Structure

```
runs/
â””â”€â”€ YYYYMMDD_HHMMSS/
    â”œâ”€â”€ summary.json          # Benchmark results summary
    â”œâ”€â”€ config.json           # Configuration used
    â”œâ”€â”€ games/                # Individual game PGN files
    â”‚   â”œâ”€â”€ gpt4o_vs_stockfish_600.pgn
    â”‚   â””â”€â”€ claude_vs_stockfish_800.pgn
    â””â”€â”€ analysis/             # Performance analysis
        â””â”€â”€ statistics.json
```

### Performance Metrics

Each bot is evaluated on:
- **Maximum ELO Reached**: Highest Stockfish rating defeated
- **Win Rate**: Percentage of games won
- **Average Game Length**: Moves per game
- **Opening Performance**: Success with different openings
- **Endgame Strength**: Performance in late-game positions

## âš™ï¸ Configuration Options

### Command Line Arguments

```bash
# Bot Configuration
--bots "provider:model:name,..."     # Custom bot specification
--preset PRESET                      # Use predefined bot lineup

# ELO Ladder Settings  
--start-elo ELO                      # Starting ELO rating (default: 600)
--elo-step STEP                      # ELO increment per rung (default: 100)  
--max-elo ELO                        # Maximum ELO to attempt (default: 2400)

# Game Settings
--think-time SECONDS                 # Thinking time per move (default: 0.3)
--max-plies COUNT                    # Maximum moves per game (default: 300)
--escalate-on MODE                   # When to advance: "always" or "on_win"

# LLM Settings
--llm-timeout SECONDS                # LLM response timeout (default: 20.0)
--llm-temperature TEMP               # Sampling temperature (default: 0.0)

# Output Settings  
--output-dir PATH                    # Results directory (default: "runs")
--save-pgn                          # Save games in PGN format

# Engine Settings
--stockfish PATH                     # Custom Stockfish executable path

# Display Options
--verbose                           # Verbose logging
--debug                             # Debug mode
--refresh-rate HZ                   # UI refresh rate (default: 6)
```

### Bot Specification Format

```
provider:model:name

Examples:
openai:gpt-4o:GPT-4o
anthropic:claude-3-5-sonnet-20241022:Claude-3.5-Sonnet  
gemini:gemini-1.5-pro:Gemini-1.5-Pro
random::Baseline
```

## ğŸ”¬ Advanced Features

### Custom Prompting

The tool uses optimized prompts for chess move generation:

```python
def _create_chess_prompt(self, board: chess.Board) -> str:
    """Create a standardized chess prompt for the LLM."""
    legal_moves = " ".join(move.uci() for move in board.legal_moves)
    color = "White" if board.turn == chess.WHITE else "Black"
    
    prompt = (
        "You are a strong chess player. Given the position and legal moves, "
        "choose the best move and respond with ONLY the UCI notation.\n\n"
        f"Position (FEN): {board.fen()}\n"
        f"Side to move: {color}\n"  
        f"Legal moves: {legal_moves}\n\n"
        "Your response must be exactly one legal UCI move."
    )
    return prompt
```

### Error Handling and Fallbacks

- **Timeout Protection**: LLM requests timeout after 20 seconds
- **Move Validation**: All moves validated against legal move list  
- **Fallback System**: Random legal moves when LLM fails
- **Robust Parsing**: Handles various move notation formats

### Concurrent Execution

Multiple bots run simultaneously with proper async handling:

```python
# Run all bots concurrently on the ELO ladder
tasks = []
for bot_spec in bot_specs:
    task = asyncio.create_task(
        self._run_bot_ladder(bot_spec, engine, output_dir)
    )
    tasks.append(task)

results = await asyncio.gather(*tasks, return_exceptions=True)
```

## ğŸ“ˆ Benchmarking Best Practices

### Model Selection
- Start with **recommended models** for reliable results
- Use **budget preset** for cost-effective testing
- Try **premium preset** for cutting-edge performance

### ELO Configuration
- Begin with **600-1000 ELO** range for initial assessment
- Use **100 ELO steps** for balanced progression
- Extend to **2400 ELO** for comprehensive evaluation

### Statistical Significance  
- Run multiple iterations for reliable statistics
- Consider different random seeds
- Analyze both win rates and maximum ELO reached

## ğŸ› ï¸ Development

### Project Structure

```
chess-llm-benchmark/
â”œâ”€â”€ chess_llm_bench/          # Main package
â”‚   â”œâ”€â”€ core/                 # Core game logic
â”‚   â”œâ”€â”€ llm/                  # LLM providers and models
â”‚   â”œâ”€â”€ ui/                   # Terminal UI components  
â”‚   â””â”€â”€ cli.py                # Command-line interface
â”œâ”€â”€ tests/                    # Test suite
â”œâ”€â”€ runs/                     # Benchmark results
â”œâ”€â”€ main.py                   # Entry point
â””â”€â”€ requirements.txt          # Dependencies
```

### Adding New Providers

1. Create a new provider class in `chess_llm_bench/llm/client.py`
2. Inherit from `BaseLLMProvider`
3. Implement `generate_move()` method
4. Register in `LLMClient.PROVIDERS`
5. Add model definitions to `chess_llm_bench/llm/models.py`

### Running Tests

```bash
# Run built-in tests
python main.py --test

# Run with pytest
pytest tests/ -v

# Type checking
mypy chess_llm_bench/

# Code formatting  
black chess_llm_bench/
isort chess_llm_bench/
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Stockfish Team** - For the amazing chess engine
- **OpenAI, Anthropic, Google** - For providing powerful language models
- **python-chess** - For excellent chess programming library
- **Rich** - For beautiful terminal formatting

## ğŸ“ Support

- ğŸ› **Issues**: [GitHub Issues](https://github.com/yourusername/chess-llm-benchmark/issues)
- ğŸ’¬ **Discussions**: [GitHub Discussions](https://github.com/yourusername/chess-llm-benchmark/discussions)  
- ğŸ“§ **Email**: your.email@example.com

---

**Made with â™Ÿï¸ and ğŸ¤– by the Chess LLM Benchmark Team**